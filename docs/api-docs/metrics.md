<!-- markdownlint-disable -->

# <kbd>module</kbd> `metrics`

Built-in evaluation suite for measuring the quality, utility, and privacy of synthetic tabular data generated by GReaT. All metrics inherit from `BaseMetric` and share a common interface:

```python
result = SomeMetric().compute(real_data, synthetic_data)
```

Column types (numerical / categorical) are auto-detected but can be passed explicitly via `num_cols` and `cat_cols`.

---

## <kbd>class</kbd> `BaseMetric`
Abstract base class for all GReaT evaluation metrics.

Subclasses must implement `name()`, `direction()`, and `compute()`.

**Methods:**

- <b>`name()`</b> → str: Human-readable metric name
- <b>`direction()`</b> → str: `"maximize"` if higher is better, `"minimize"` if lower is better
- <b>`compute(real_data, synthetic_data, **kwargs)`</b> → dict: Compute the metric

---

## Statistical Metrics

### <kbd>class</kbd> `ColumnShapes`
Per-column distribution similarity.

Uses the Kolmogorov-Smirnov test for numerical columns and Total Variation Distance for categorical columns. Returns a score in [0, 1] per column — 1.0 means identical distributions.

```python
from be_great.metrics import ColumnShapes

result = ColumnShapes().compute(real_data, synthetic_data)
# result["column_shapes_mean"]   -> average similarity across all columns
# result["column_shapes_std"]    -> standard deviation
# result["column_shapes_detail"] -> per-column scores dict
```

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

### <kbd>class</kbd> `ColumnPairTrends`
Pairwise correlation preservation.

Compares Pearson correlations for numerical pairs and Cramer's V for categorical pairs between real and synthetic data. Returns a score in [0, 1] — 1.0 means identical pairwise relationships.

```python
from be_great.metrics import ColumnPairTrends

result = ColumnPairTrends().compute(real_data, synthetic_data)
# result["column_pair_trends_mean"]        -> overall similarity
# result["column_pair_trends_numerical"]   -> numerical pair similarity
# result["column_pair_trends_categorical"] -> categorical pair similarity
```

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

### <kbd>class</kbd> `BasicStatistics`
Summary statistics comparison.

Compares mean, standard deviation, and median for numerical columns, and category frequency distributions for categorical columns.

```python
from be_great.metrics import BasicStatistics

result = BasicStatistics().compute(real_data, synthetic_data)
# result["basic_statistics"]["col_name"]["real_mean"]
# result["basic_statistics"]["col_name"]["synth_mean"]
# result["basic_statistics"]["col_name"]["mean_diff_pct"]
```

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

## Fidelity & Utility Metrics

### <kbd>class</kbd> `DiscriminatorMetric`
Trains a Random Forest classifier to distinguish real from synthetic data.

A score close to 0.5 means the synthetic data is indistinguishable from real data. A score close to 1.0 means the classifier easily tells them apart. Uses cross-validated hyperparameter tuning and reports mean/std over multiple random seeds.

```python
from be_great.metrics import DiscriminatorMetric

result = DiscriminatorMetric(n_runs=10).compute(real_data, synthetic_data)
# result["discriminator_mean"] -> mean accuracy (0.5 = best)
# result["discriminator_std"]  -> standard deviation
```

**Args (\_\_init\_\_):**

- <b>`metric`</b> (callable): Scoring function. Default: `accuracy_score`
- <b>`n_runs`</b> (int): Number of evaluation runs. Default: 10
- <b>`encoder`</b> (type): Encoder for categorical features. Default: `OrdinalEncoder`
- <b>`encoder_params`</b> (dict, optional): Encoder parameters

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`cat_cols`</b> (list, optional): Categorical column names
- <b>`test_ratio`</b> (float): Fraction used for testing. Default: 0.2
- <b>`cv`</b> (int): Cross-validation folds. Default: 5

---

### <kbd>class</kbd> `MLEfficiency`
Machine learning efficiency — train on synthetic, test on real.

Measures the downstream utility of synthetic data. A model is trained entirely on the synthetic dataset and evaluated on a held-out real test set. The closer the score is to the performance achieved when training on real data, the higher the utility.

```python
from be_great.metrics import MLEfficiency
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

result = MLEfficiency(
    model=RandomForestClassifier,
    metric=accuracy_score,
    model_params={"n_estimators": 100},
).compute(real_data, synthetic_data, label_col="target")
# result["mle_mean"]   -> mean score across seeds
# result["mle_std"]    -> standard deviation
# result["mle_scores"] -> per-seed scores list
```

**Args (\_\_init\_\_):**

- <b>`model`</b> (type): Sklearn-compatible model class
- <b>`metric`</b> (callable): Scoring function
- <b>`model_params`</b> (dict, optional): Model constructor parameters
- <b>`encoder`</b> (type): Encoder for categorical features. Default: `OrdinalEncoder`
- <b>`encoder_params`</b> (dict, optional): Encoder parameters
- <b>`normalize`</b> (bool): Standard-scale continuous features. Default: False
- <b>`use_proba`</b> (bool): Use `predict_proba` instead of `predict`. Default: False
- <b>`metric_params`</b> (dict, optional): Extra kwargs for the scoring function

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original training dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset (used for training)
- <b>`label_col`</b> (str): Target column name
- <b>`cat_cols`</b> (list, optional): Categorical column names
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`real_test_data`</b> (DataFrame, optional): Separate real test set
- <b>`test_ratio`</b> (float): Split ratio if no separate test set. Default: 0.2
- <b>`random_seeds`</b> (list[int], optional): Seeds for multiple runs. Default: [512, 13, 23, 28, 21]

---

## Privacy Metrics

### <kbd>class</kbd> `DistanceToClosestRecord`
Distance to Closest Record (DCR).

For each synthetic record, computes the distance to the closest record in the real dataset. Uses L1 (Manhattan) distance for numerical features and Hamming distance for categorical features. Records with distance 0 are exact copies.

```python
from be_great.metrics import DistanceToClosestRecord

result = DistanceToClosestRecord().compute(real_data, synthetic_data)
# result["dcr_mean"]      -> mean minimum distance
# result["dcr_std"]       -> standard deviation
# result["n_copies"]      -> number of exact copies
# result["ratio_copies"]  -> fraction of exact copies
```

**Args (\_\_init\_\_):**

- <b>`n_samples`</b> (int): Number of synthetic samples to evaluate. 0 = use all. Default: 0
- <b>`use_euclidean`</b> (bool): Use L2 norm instead of L1 for numerical features. Default: False

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

### <kbd>class</kbd> `kAnonymization`
k-Anonymization metric.

Evaluates the k-anonymity of a dataset using KMeans clustering. Each record should be similar to at least k-1 other records on the quasi-identifying variables. Reports the ratio `k_synthetic / k_real` — a ratio >= 1 means the synthetic data has at least as much k-anonymity as the real data.

```python
from be_great.metrics import kAnonymization

result = kAnonymization().compute(real_data, synthetic_data)
# result["k_real"]      -> k value for original data
# result["k_synthetic"] -> k value for synthetic data
# result["k_ratio"]     -> syn / real ratio
```

**Args (\_\_init\_\_):**

- <b>`n_clusters_list`</b> (list[int], optional): Cluster counts to evaluate. Default: [2, 5, 10, 15]

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`sensitive_cols`</b> (list, optional): Columns to exclude from quasi-identifiers

---

### <kbd>class</kbd> `lDiversity`
l-Diversity metric.

Measures the diversity of sensitive attribute values within each equivalence class. Uses KMeans to form groups and checks how many distinct sensitive values exist in the smallest group. Higher l-diversity means better protection against attribute inference.

```python
from be_great.metrics import lDiversity

result = lDiversity(sensitive_col="diagnosis").compute(real_data, synthetic_data)
# result["l_real"]      -> l value for original data
# result["l_synthetic"] -> l value for synthetic data
# result["l_ratio"]     -> syn / real ratio
```

**Args (\_\_init\_\_):**

- <b>`sensitive_col`</b> (str): Name of the sensitive attribute column
- <b>`n_clusters_list`</b> (list[int], optional): Cluster counts to evaluate. Default: [2, 5, 10, 15]

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset

---

### <kbd>class</kbd> `IdentifiabilityScore`
Identifiability score.

Measures the risk that a synthetic record can be linked back to a specific real record. Uses k-nearest neighbors and checks whether the closest real neighbor is significantly closer than the second closest (distance ratio below threshold).

```python
from be_great.metrics import IdentifiabilityScore

result = IdentifiabilityScore().compute(real_data, synthetic_data)
# result["identifiability_score"] -> fraction of identifiable records
# result["mean_distance_ratio"]   -> average d1/d2 ratio
```

**Args (\_\_init\_\_):**

- <b>`n_neighbors`</b> (int): Number of nearest neighbors. Default: 5
- <b>`threshold_ratio`</b> (float): Identifiability threshold. Default: 0.5

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

### <kbd>class</kbd> `DeltaPresence`
Delta-presence metric.

Measures how much the presence of an individual in the dataset can be inferred from the synthetic data. Computes the fraction of real records that have a near-exact match in the synthetic dataset within a distance threshold.

```python
from be_great.metrics import DeltaPresence

result = DeltaPresence(threshold=0.5).compute(real_data, synthetic_data)
# result["delta_presence"]        -> fraction of real records with a match
# result["mean_nearest_distance"] -> average nearest distance
```

**Args (\_\_init\_\_):**

- <b>`threshold`</b> (float): Distance threshold. 0.0 = exact match only. Default: 0.0

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original dataset
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names

---

### <kbd>class</kbd> `MembershipInference`
Membership inference risk.

Simulates a membership inference attack: given a record, can an attacker determine whether it was in the training set? Compares distances from known-member records (train) and known-non-member records (holdout) to their nearest synthetic neighbors.

A score close to 0.5 means the attacker cannot distinguish members from non-members (good privacy). A score close to 1.0 means high membership inference risk.

```python
from be_great.metrics import MembershipInference

result = MembershipInference().compute(real_data, synthetic_data)
# result["membership_inference_score"] -> attacker accuracy
# result["mean_member_distance"]       -> avg distance for members
# result["mean_non_member_distance"]   -> avg distance for non-members
```

**Args (\_\_init\_\_):**

- <b>`n_neighbors`</b> (int): Number of nearest neighbors. Default: 1

**Args (compute):**

- <b>`real_data`</b> (DataFrame): Original training dataset (members)
- <b>`synthetic_data`</b> (DataFrame): Generated dataset
- <b>`holdout_data`</b> (DataFrame, optional): Non-member data. If None, real_data is split.
- <b>`num_cols`</b> (list, optional): Numerical column names
- <b>`cat_cols`</b> (list, optional): Categorical column names
- <b>`holdout_ratio`</b> (float): Split ratio if no holdout provided. Default: 0.5

---

_This file was manually authored following the [lazydocs](https://github.com/ml-tooling/lazydocs) convention._
